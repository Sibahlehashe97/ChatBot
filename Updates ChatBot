import streamlit as st
import pandas as pd
import subprocess
import textwrap
import re
import datetime

st.set_page_config(page_title="Ask Your Excel File", layout="centered")
st.title("📊 Ask Questions About Your Employee Data")

# Step 1: Upload Excel and convert to DataFrame
uploaded_file = st.file_uploader("Upload your employee Excel file", type=["xlsx"])

if uploaded_file:
    df = pd.read_excel(uploaded_file)
    st.success("✅ Data loaded successfully!")
    st.write("### Preview of your dataset:", df.head())

    # Step 2: Ask natural language question
    user_question = st.text_input("💬 Ask a question about the dataset:",
                                  placeholder="e.g., How much did we spend on salaries?")

    if user_question:
        # Step 3: Build LLM prompt
        column_info = ", ".join(df.columns)
        sample_data = df.head(3).to_dict(orient="records")

        prompt = f"""
You are a Python data analyst. Given a DataFrame called `df` with these columns: {column_info}
Here are a few sample rows from the dataset: {sample_data}

The user asked: "{user_question}"

Write a Python function named `answer(df)` that answers the question using pandas.
Only return the code with the function definition and no explanations or markdown.ANd no importing any libraries just the function only
DateNotified and DateClosed are date type columns. With users queries relating to datetime, make sure you import datetimr inside the function such as in the following function : def answer(df):
    from datetime import datetime  # Local import
    start_date = datetime(2024, 9, 1)
    end_date = datetime(2024, 12, 31)
    return df[(df['DateNotified'] >= start_date) & (df['DateNotified'] <= end_date)]
        """

        st.subheader("🔧 Generated Code")
        st.write("Asking LLaMA2...")

        # Step 4: Call local LLaMA2 with Ollama (capture stdout)
        try:
            # API call to local Ollama server
            def call_ollama(prompt):
                import requests
                url = "http://localhost:11434/api/generate"
                headers = {"Content-Type": "application/json"}
                data = {
                    "model": "codellama",  # or llama2 if you prefer
                    "prompt": prompt,
                    "stream": False
                }

                response = requests.post(url, headers=headers, json=data)
                response.raise_for_status()
                return response.json()["response"]

            raw_code = call_ollama(prompt).strip()

            # Step 5: Remove triple backticks and clean up
            clean_code = re.sub(r"```(?:python)?|```", "", raw_code).strip()

            # Step 6: Display the function
            st.code(clean_code, language="python")

            # Step 7: Execute the code
            global_vars = {"pd": pd, "datetime": datetime, "Timestamp": pd.Timestamp}
            local_vars = {}

            exec(clean_code, global_vars, local_vars)

            if "answer" in local_vars:
                output = local_vars["answer"](df)
                st.subheader("📌 Result")
                st.write(output)
            else:
                st.error("❌ No 'answer' function was defined in the code.")

        except Exception as e:
            st.error("❌ Error: " + str(e))
